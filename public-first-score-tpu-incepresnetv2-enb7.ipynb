{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# if you like it ,please vote for it,good score for you ( •̀ ω •́ )✧"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "V1: seresnet101 0.959\n",
    "\n",
    "V6: B7+InceprtionresnetV2 0.974\n",
    "\n",
    "V7: image_size=800\n",
    "\n",
    "V8:replace optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q efficientnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "import efficientnet.tfkeras as efn\n",
    "from kaggle_datasets import KaggleDatasets\n",
    "from tensorflow.keras.models import Sequential\n",
    "import tensorflow.keras.layers as L\n",
    "from tensorflow.keras.applications import InceptionResNetV2\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TPU Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "AUTO = tf.data.experimental.AUTOTUNE\n",
    "\n",
    "# Create strategy from tpu\n",
    "tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
    "tf.config.experimental_connect_to_cluster(tpu)\n",
    "tf.tpu.experimental.initialize_tpu_system(tpu)\n",
    "strategy = tf.distribute.experimental.TPUStrategy(tpu)\n",
    "\n",
    "# Data access\n",
    "GCS_DS_PATH = KaggleDatasets().get_gcs_path()\n",
    "\n",
    "# Configuration\n",
    "EPOCHS = 40\n",
    "BATCH_SIZE = 16 * strategy.num_replicas_in_sync"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_path(st):\n",
    "    return GCS_DS_PATH + '/images/' + st + '.jpg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('/kaggle/input/plant-pathology-2020-fgvc7/train.csv')\n",
    "test = pd.read_csv('/kaggle/input/plant-pathology-2020-fgvc7/test.csv')\n",
    "sub = pd.read_csv('/kaggle/input/plant-pathology-2020-fgvc7/sample_submission.csv')\n",
    "\n",
    "train_paths = train.image_id.apply(format_path).values\n",
    "test_paths = test.image_id.apply(format_path).values\n",
    "\n",
    "train_labels = train.loc[:, 'healthy':].values\n",
    "\n",
    "## если планируете обучать модель с валидирующим набором данных\n",
    "# train_paths, valid_paths, train_labels, valid_labels = train_test_split(\n",
    "#     train_paths, train_labels, test_size=0.15, random_state=2020)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Resizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_size = 800"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_image(filename, label=None, image_size=(image_size, image_size)):\n",
    "    bits = tf.io.read_file(filename)\n",
    "    image = tf.image.decode_jpeg(bits, channels=3)\n",
    "    image = tf.cast(image, tf.float32) / 255.0\n",
    "    image = tf.image.resize(image, image_size)\n",
    "    \n",
    "    if label is None:\n",
    "        return image\n",
    "    else:\n",
    "        return image, label\n",
    "\n",
    "def data_augment(image, label=None):\n",
    "    image = tf.image.random_flip_left_right(image)\n",
    "    image = tf.image.random_flip_up_down(image)\n",
    "    \n",
    "    if label is None:\n",
    "        return image\n",
    "    else:\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = (\n",
    "tf.data.Dataset\n",
    "    .from_tensor_slices((train_paths, train_labels))\n",
    "    .map(decode_image, num_parallel_calls=AUTO)\n",
    "    .cache()\n",
    "    .map(data_augment, num_parallel_calls=AUTO)\n",
    "    .repeat()\n",
    "    .shuffle(512)\n",
    "    .batch(BATCH_SIZE)\n",
    "    .prefetch(AUTO)\n",
    ")\n",
    "train_dataset_1 = (\n",
    "tf.data.Dataset\n",
    "    .from_tensor_slices((train_paths, train_labels))\n",
    "    .map(decode_image, num_parallel_calls=AUTO)\n",
    "    .cache()\n",
    "    .map(data_augment, num_parallel_calls=AUTO)\n",
    "    .repeat()\n",
    "    .shuffle(512)\n",
    "    .batch(64)\n",
    "    .prefetch(AUTO)\n",
    ")\n",
    "# valid_dataset = (\n",
    "#     tf.data.Dataset\n",
    "#     .from_tensor_slices((valid_paths, valid_labels))\n",
    "#     .map(decode_image, num_parallel_calls=AUTO)\n",
    "#     .batch(BATCH_SIZE)\n",
    "#     .cache()\n",
    "#     .prefetch(AUTO)\n",
    "# )\n",
    "\n",
    "test_dataset = (\n",
    "    tf.data.Dataset\n",
    "    .from_tensor_slices(test_paths)\n",
    "    .map(decode_image, num_parallel_calls=AUTO)\n",
    "    .batch(BATCH_SIZE)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate schedule: 0.0001 to 0.0004 to 0.0001\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "LR_START = 0.0001\n",
    "LR_MAX = 0.00005 * strategy.num_replicas_in_sync\n",
    "LR_MIN = 0.0001\n",
    "LR_RAMPUP_EPOCHS = 4\n",
    "LR_SUSTAIN_EPOCHS = 6\n",
    "LR_EXP_DECAY = .8\n",
    "\n",
    "def lrfn(epoch):\n",
    "    if epoch < LR_RAMPUP_EPOCHS:\n",
    "        lr = (LR_MAX - LR_START) / LR_RAMPUP_EPOCHS * epoch + LR_START\n",
    "    elif epoch < LR_RAMPUP_EPOCHS + LR_SUSTAIN_EPOCHS:\n",
    "        lr = LR_MAX\n",
    "    else:\n",
    "        lr = (LR_MAX - LR_MIN) * LR_EXP_DECAY**(epoch - LR_RAMPUP_EPOCHS - LR_SUSTAIN_EPOCHS) + LR_MIN\n",
    "    return lr\n",
    "    \n",
    "lr_callback = tf.keras.callbacks.LearningRateScheduler(lrfn, verbose=True)\n",
    "\n",
    "rng = [i for i in range(EPOCHS)]\n",
    "y = [lrfn(x) for x in rng]\n",
    "plt.plot(rng, y)\n",
    "print(\"Learning rate schedule: {:.3g} to {:.3g} to {:.3g}\".format(y[0], max(y), y[-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://github.com/fchollet/deep-learning-models/releases/download/v0.7/inception_resnet_v2_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "219062272/219055592 [==============================] - 7s 0us/step\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "inception_resnet_v2 (Model)  (None, 23, 23, 1536)      54336736  \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d (Gl (None, 1536)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 4)                 6148      \n",
      "=================================================================\n",
      "Total params: 54,342,884\n",
      "Trainable params: 54,282,340\n",
      "Non-trainable params: 60,544\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "with strategy.scope():\n",
    "    model = tf.keras.Sequential([\n",
    "        InceptionResNetV2(\n",
    "            input_shape=(image_size, image_size, 3),\n",
    "            weights='imagenet',\n",
    "            include_top=False\n",
    "        ),\n",
    "        L.GlobalAveragePooling2D(),\n",
    "        L.Dense(train_labels.shape[1], activation='softmax')\n",
    "    ])\n",
    "        \n",
    "    model.compile(\n",
    "        optimizer = 'adam',\n",
    "        loss = 'categorical_crossentropy',\n",
    "        metrics=['categorical_accuracy']\n",
    "    )\n",
    "    model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train for 14 steps\n",
      "\n",
      "Epoch 00001: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 1/40\n",
      "14/14 [==============================] - 283s 20s/step - loss: 0.8944 - categorical_accuracy: 0.6842\n",
      "\n",
      "Epoch 00002: LearningRateScheduler reducing learning rate to 0.00017500000000000003.\n",
      "Epoch 2/40\n",
      "14/14 [==============================] - 16s 1s/step - loss: 0.2656 - categorical_accuracy: 0.9079\n",
      "\n",
      "Epoch 00003: LearningRateScheduler reducing learning rate to 0.00025.\n",
      "Epoch 3/40\n",
      "14/14 [==============================] - 16s 1s/step - loss: 0.1431 - categorical_accuracy: 0.9542\n",
      "\n",
      "Epoch 00004: LearningRateScheduler reducing learning rate to 0.00032500000000000004.\n",
      "Epoch 4/40\n",
      "14/14 [==============================] - 15s 1s/step - loss: 0.1146 - categorical_accuracy: 0.9621\n",
      "\n",
      "Epoch 00005: LearningRateScheduler reducing learning rate to 0.0004.\n",
      "Epoch 5/40\n",
      "14/14 [==============================] - 16s 1s/step - loss: 0.1014 - categorical_accuracy: 0.9665\n",
      "\n",
      "Epoch 00006: LearningRateScheduler reducing learning rate to 0.0004.\n",
      "Epoch 6/40\n",
      "14/14 [==============================] - 16s 1s/step - loss: 0.0837 - categorical_accuracy: 0.9704\n",
      "\n",
      "Epoch 00007: LearningRateScheduler reducing learning rate to 0.0004.\n",
      "Epoch 7/40\n",
      "14/14 [==============================] - 16s 1s/step - loss: 0.0818 - categorical_accuracy: 0.9721\n",
      "\n",
      "Epoch 00008: LearningRateScheduler reducing learning rate to 0.0004.\n",
      "Epoch 8/40\n",
      "14/14 [==============================] - 16s 1s/step - loss: 0.0676 - categorical_accuracy: 0.9766\n",
      "\n",
      "Epoch 00009: LearningRateScheduler reducing learning rate to 0.0004.\n",
      "Epoch 9/40\n",
      "14/14 [==============================] - 16s 1s/step - loss: 0.0527 - categorical_accuracy: 0.9855\n",
      "\n",
      "Epoch 00010: LearningRateScheduler reducing learning rate to 0.0004.\n",
      "Epoch 10/40\n",
      "14/14 [==============================] - 16s 1s/step - loss: 0.0402 - categorical_accuracy: 0.9866\n",
      "\n",
      "Epoch 00011: LearningRateScheduler reducing learning rate to 0.0004.\n",
      "Epoch 11/40\n",
      "14/14 [==============================] - 16s 1s/step - loss: 0.0390 - categorical_accuracy: 0.9905\n",
      "\n",
      "Epoch 00012: LearningRateScheduler reducing learning rate to 0.00034.\n",
      "Epoch 12/40\n",
      "14/14 [==============================] - 16s 1s/step - loss: 0.0306 - categorical_accuracy: 0.9888\n",
      "\n",
      "Epoch 00013: LearningRateScheduler reducing learning rate to 0.00029200000000000005.\n",
      "Epoch 13/40\n",
      "14/14 [==============================] - 16s 1s/step - loss: 0.0285 - categorical_accuracy: 0.9900\n",
      "\n",
      "Epoch 00014: LearningRateScheduler reducing learning rate to 0.00025360000000000004.\n",
      "Epoch 14/40\n",
      "14/14 [==============================] - 16s 1s/step - loss: 0.0169 - categorical_accuracy: 0.9944\n",
      "\n",
      "Epoch 00015: LearningRateScheduler reducing learning rate to 0.00022288000000000006.\n",
      "Epoch 15/40\n",
      "14/14 [==============================] - 16s 1s/step - loss: 0.0190 - categorical_accuracy: 0.9944\n",
      "\n",
      "Epoch 00016: LearningRateScheduler reducing learning rate to 0.00019830400000000006.\n",
      "Epoch 16/40\n",
      "14/14 [==============================] - 16s 1s/step - loss: 0.0198 - categorical_accuracy: 0.9950\n",
      "\n",
      "Epoch 00017: LearningRateScheduler reducing learning rate to 0.00017864320000000004.\n",
      "Epoch 17/40\n",
      "14/14 [==============================] - 16s 1s/step - loss: 0.0179 - categorical_accuracy: 0.9944\n",
      "\n",
      "Epoch 00018: LearningRateScheduler reducing learning rate to 0.00016291456000000005.\n",
      "Epoch 18/40\n",
      "14/14 [==============================] - 16s 1s/step - loss: 0.0091 - categorical_accuracy: 0.9978\n",
      "\n",
      "Epoch 00019: LearningRateScheduler reducing learning rate to 0.00015033164800000003.\n",
      "Epoch 19/40\n",
      "14/14 [==============================] - 16s 1s/step - loss: 0.0048 - categorical_accuracy: 0.9994\n",
      "\n",
      "Epoch 00020: LearningRateScheduler reducing learning rate to 0.00014026531840000004.\n",
      "Epoch 20/40\n",
      "14/14 [==============================] - 16s 1s/step - loss: 0.0050 - categorical_accuracy: 0.9989\n",
      "\n",
      "Epoch 00021: LearningRateScheduler reducing learning rate to 0.00013221225472000002.\n",
      "Epoch 21/40\n",
      "14/14 [==============================] - 16s 1s/step - loss: 0.0023 - categorical_accuracy: 0.9994\n",
      "\n",
      "Epoch 00022: LearningRateScheduler reducing learning rate to 0.00012576980377600002.\n",
      "Epoch 22/40\n",
      "14/14 [==============================] - 16s 1s/step - loss: 0.0022 - categorical_accuracy: 1.0000\n",
      "\n",
      "Epoch 00023: LearningRateScheduler reducing learning rate to 0.00012061584302080001.\n",
      "Epoch 23/40\n",
      "14/14 [==============================] - 16s 1s/step - loss: 0.0018 - categorical_accuracy: 1.0000\n",
      "\n",
      "Epoch 00024: LearningRateScheduler reducing learning rate to 0.00011649267441664002.\n",
      "Epoch 24/40\n",
      "14/14 [==============================] - 16s 1s/step - loss: 0.0020 - categorical_accuracy: 1.0000\n",
      "\n",
      "Epoch 00025: LearningRateScheduler reducing learning rate to 0.00011319413953331202.\n",
      "Epoch 25/40\n",
      "14/14 [==============================] - 16s 1s/step - loss: 0.0044 - categorical_accuracy: 0.9989\n",
      "\n",
      "Epoch 00026: LearningRateScheduler reducing learning rate to 0.00011055531162664962.\n",
      "Epoch 26/40\n",
      "14/14 [==============================] - 16s 1s/step - loss: 0.0023 - categorical_accuracy: 0.9994\n",
      "\n",
      "Epoch 00027: LearningRateScheduler reducing learning rate to 0.0001084442493013197.\n",
      "Epoch 27/40\n",
      "14/14 [==============================] - 16s 1s/step - loss: 0.0048 - categorical_accuracy: 0.9994\n",
      "\n",
      "Epoch 00028: LearningRateScheduler reducing learning rate to 0.00010675539944105576.\n",
      "Epoch 28/40\n",
      "14/14 [==============================] - 16s 1s/step - loss: 0.0020 - categorical_accuracy: 0.9994\n",
      "\n",
      "Epoch 00029: LearningRateScheduler reducing learning rate to 0.0001054043195528446.\n",
      "Epoch 29/40\n",
      "14/14 [==============================] - 16s 1s/step - loss: 0.0021 - categorical_accuracy: 1.0000\n",
      "\n",
      "Epoch 00030: LearningRateScheduler reducing learning rate to 0.00010432345564227568.\n",
      "Epoch 30/40\n",
      "14/14 [==============================] - 16s 1s/step - loss: 0.0029 - categorical_accuracy: 0.9989\n",
      "\n",
      "Epoch 00031: LearningRateScheduler reducing learning rate to 0.00010345876451382055.\n",
      "Epoch 31/40\n",
      "14/14 [==============================] - 16s 1s/step - loss: 6.3426e-04 - categorical_accuracy: 1.0000\n",
      "\n",
      "Epoch 00032: LearningRateScheduler reducing learning rate to 0.00010276701161105644.\n",
      "Epoch 32/40\n",
      "14/14 [==============================] - 16s 1s/step - loss: 0.0045 - categorical_accuracy: 0.9983\n",
      "\n",
      "Epoch 00033: LearningRateScheduler reducing learning rate to 0.00010221360928884516.\n",
      "Epoch 33/40\n",
      "14/14 [==============================] - 16s 1s/step - loss: 0.0010 - categorical_accuracy: 1.0000\n",
      "\n",
      "Epoch 00034: LearningRateScheduler reducing learning rate to 0.00010177088743107613.\n",
      "Epoch 34/40\n",
      "14/14 [==============================] - 16s 1s/step - loss: 7.9806e-04 - categorical_accuracy: 1.0000\n",
      "\n",
      "Epoch 00035: LearningRateScheduler reducing learning rate to 0.0001014167099448609.\n",
      "Epoch 35/40\n",
      "14/14 [==============================] - 16s 1s/step - loss: 0.0013 - categorical_accuracy: 1.0000\n",
      "\n",
      "Epoch 00036: LearningRateScheduler reducing learning rate to 0.00010113336795588872.\n",
      "Epoch 36/40\n",
      "14/14 [==============================] - 16s 1s/step - loss: 0.0024 - categorical_accuracy: 0.9994\n",
      "\n",
      "Epoch 00037: LearningRateScheduler reducing learning rate to 0.00010090669436471098.\n",
      "Epoch 37/40\n",
      "14/14 [==============================] - 16s 1s/step - loss: 0.0012 - categorical_accuracy: 1.0000\n",
      "\n",
      "Epoch 00038: LearningRateScheduler reducing learning rate to 0.00010072535549176879.\n",
      "Epoch 38/40\n",
      "14/14 [==============================] - 16s 1s/step - loss: 0.0017 - categorical_accuracy: 0.9994\n",
      "\n",
      "Epoch 00039: LearningRateScheduler reducing learning rate to 0.00010058028439341503.\n",
      "Epoch 39/40\n",
      "14/14 [==============================] - 16s 1s/step - loss: 0.0037 - categorical_accuracy: 0.9989\n",
      "\n",
      "Epoch 00040: LearningRateScheduler reducing learning rate to 0.00010046422751473202.\n",
      "Epoch 40/40\n",
      "14/14 [==============================] - 16s 1s/step - loss: 0.0015 - categorical_accuracy: 0.9994\n"
     ]
    }
   ],
   "source": [
    "\n",
    "STEPS_PER_EPOCH = train_labels.shape[0] // BATCH_SIZE\n",
    "\n",
    "history = model.fit(\n",
    "    train_dataset, \n",
    "    epochs=EPOCHS, \n",
    "    callbacks=[lr_callback],\n",
    "    steps_per_epoch=STEPS_PER_EPOCH,\n",
    "    #validation_data=valid_dataset\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://github.com/Callidior/keras-applications/releases/download/efficientnet/efficientnet-b7_weights_tf_dim_ordering_tf_kernels_autoaugment_notop.h5\n",
      "258441216/258434480 [==============================] - 8s 0us/step\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "efficientnet-b7 (Model)      (None, 25, 25, 2560)      64097680  \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d_1 ( (None, 2560)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 4)                 10244     \n",
      "=================================================================\n",
      "Total params: 64,107,924\n",
      "Trainable params: 63,797,204\n",
      "Non-trainable params: 310,720\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "with strategy.scope():\n",
    "    model2 = tf.keras.Sequential([\n",
    "        efn.EfficientNetB7(\n",
    "            input_shape=(image_size, image_size, 3),\n",
    "            weights='imagenet',\n",
    "            include_top=False\n",
    "        ),\n",
    "        L.GlobalAveragePooling2D(),\n",
    "        L.Dense(train_labels.shape[1], activation='softmax')\n",
    "    ])\n",
    "        \n",
    "    model2.compile(\n",
    "        optimizer = 'adam',\n",
    "        loss = 'categorical_crossentropy',\n",
    "        metrics=['categorical_accuracy']\n",
    "    )\n",
    "    model2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train for 28 steps\n",
      "\n",
      "Epoch 00001: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 1/40\n",
      "28/28 [==============================] - 340s 12s/step - loss: 0.9177 - categorical_accuracy: 0.7003\n",
      "\n",
      "Epoch 00002: LearningRateScheduler reducing learning rate to 0.00017500000000000003.\n",
      "Epoch 2/40\n",
      "28/28 [==============================] - 32s 1s/step - loss: 0.3756 - categorical_accuracy: 0.8823\n",
      "\n",
      "Epoch 00003: LearningRateScheduler reducing learning rate to 0.00025.\n",
      "Epoch 3/40\n",
      "28/28 [==============================] - 32s 1s/step - loss: 0.2744 - categorical_accuracy: 0.9135\n",
      "\n",
      "Epoch 00004: LearningRateScheduler reducing learning rate to 0.00032500000000000004.\n",
      "Epoch 4/40\n",
      "28/28 [==============================] - 32s 1s/step - loss: 0.2298 - categorical_accuracy: 0.9230\n",
      "\n",
      "Epoch 00005: LearningRateScheduler reducing learning rate to 0.0004.\n",
      "Epoch 5/40\n",
      "28/28 [==============================] - 32s 1s/step - loss: 0.2245 - categorical_accuracy: 0.9269\n",
      "\n",
      "Epoch 00006: LearningRateScheduler reducing learning rate to 0.0004.\n",
      "Epoch 6/40\n",
      "28/28 [==============================] - 32s 1s/step - loss: 0.1895 - categorical_accuracy: 0.9414\n",
      "\n",
      "Epoch 00007: LearningRateScheduler reducing learning rate to 0.0004.\n",
      "Epoch 7/40\n",
      "28/28 [==============================] - 32s 1s/step - loss: 0.1716 - categorical_accuracy: 0.9470\n",
      "\n",
      "Epoch 00008: LearningRateScheduler reducing learning rate to 0.0004.\n",
      "Epoch 8/40\n",
      "28/28 [==============================] - 32s 1s/step - loss: 0.1141 - categorical_accuracy: 0.9621\n",
      "\n",
      "Epoch 00009: LearningRateScheduler reducing learning rate to 0.0004.\n",
      "Epoch 9/40\n",
      "28/28 [==============================] - 32s 1s/step - loss: 0.1453 - categorical_accuracy: 0.9548\n",
      "\n",
      "Epoch 00010: LearningRateScheduler reducing learning rate to 0.0004.\n",
      "Epoch 10/40\n",
      "28/28 [==============================] - 32s 1s/step - loss: 0.0833 - categorical_accuracy: 0.9788\n",
      "\n",
      "Epoch 00011: LearningRateScheduler reducing learning rate to 0.0004.\n",
      "Epoch 11/40\n",
      "28/28 [==============================] - 32s 1s/step - loss: 0.0733 - categorical_accuracy: 0.9794\n",
      "\n",
      "Epoch 00012: LearningRateScheduler reducing learning rate to 0.00034.\n",
      "Epoch 12/40\n",
      "28/28 [==============================] - 32s 1s/step - loss: 0.0628 - categorical_accuracy: 0.9788\n",
      "\n",
      "Epoch 00013: LearningRateScheduler reducing learning rate to 0.00029200000000000005.\n",
      "Epoch 13/40\n",
      "28/28 [==============================] - 32s 1s/step - loss: 0.0389 - categorical_accuracy: 0.9888\n",
      "\n",
      "Epoch 00014: LearningRateScheduler reducing learning rate to 0.00025360000000000004.\n",
      "Epoch 14/40\n",
      "28/28 [==============================] - 32s 1s/step - loss: 0.0413 - categorical_accuracy: 0.9894\n",
      "\n",
      "Epoch 00015: LearningRateScheduler reducing learning rate to 0.00022288000000000006.\n",
      "Epoch 15/40\n",
      "28/28 [==============================] - 32s 1s/step - loss: 0.0214 - categorical_accuracy: 0.9944\n",
      "\n",
      "Epoch 00016: LearningRateScheduler reducing learning rate to 0.00019830400000000006.\n",
      "Epoch 16/40\n",
      "28/28 [==============================] - 33s 1s/step - loss: 0.0195 - categorical_accuracy: 0.9939\n",
      "\n",
      "Epoch 00017: LearningRateScheduler reducing learning rate to 0.00017864320000000004.\n",
      "Epoch 17/40\n",
      "28/28 [==============================] - 32s 1s/step - loss: 0.0251 - categorical_accuracy: 0.9922\n",
      "\n",
      "Epoch 00018: LearningRateScheduler reducing learning rate to 0.00016291456000000005.\n",
      "Epoch 18/40\n",
      "28/28 [==============================] - 32s 1s/step - loss: 0.0178 - categorical_accuracy: 0.9961\n",
      "\n",
      "Epoch 00019: LearningRateScheduler reducing learning rate to 0.00015033164800000003.\n",
      "Epoch 19/40\n",
      "28/28 [==============================] - 32s 1s/step - loss: 0.0169 - categorical_accuracy: 0.9955\n",
      "\n",
      "Epoch 00020: LearningRateScheduler reducing learning rate to 0.00014026531840000004.\n",
      "Epoch 20/40\n",
      "28/28 [==============================] - 33s 1s/step - loss: 0.0155 - categorical_accuracy: 0.9961\n",
      "\n",
      "Epoch 00021: LearningRateScheduler reducing learning rate to 0.00013221225472000002.\n",
      "Epoch 21/40\n",
      "28/28 [==============================] - 32s 1s/step - loss: 0.0139 - categorical_accuracy: 0.9967\n",
      "\n",
      "Epoch 00022: LearningRateScheduler reducing learning rate to 0.00012576980377600002.\n",
      "Epoch 22/40\n",
      "28/28 [==============================] - 32s 1s/step - loss: 0.0087 - categorical_accuracy: 0.9978\n",
      "\n",
      "Epoch 00023: LearningRateScheduler reducing learning rate to 0.00012061584302080001.\n",
      "Epoch 23/40\n",
      "28/28 [==============================] - 32s 1s/step - loss: 0.0166 - categorical_accuracy: 0.9967\n",
      "\n",
      "Epoch 00024: LearningRateScheduler reducing learning rate to 0.00011649267441664002.\n",
      "Epoch 24/40\n",
      "28/28 [==============================] - 33s 1s/step - loss: 0.0061 - categorical_accuracy: 0.9989\n",
      "\n",
      "Epoch 00025: LearningRateScheduler reducing learning rate to 0.00011319413953331202.\n",
      "Epoch 25/40\n",
      "28/28 [==============================] - 32s 1s/step - loss: 0.0102 - categorical_accuracy: 0.9978\n",
      "\n",
      "Epoch 00026: LearningRateScheduler reducing learning rate to 0.00011055531162664962.\n",
      "Epoch 26/40\n",
      "28/28 [==============================] - 33s 1s/step - loss: 0.0114 - categorical_accuracy: 0.9967\n",
      "\n",
      "Epoch 00027: LearningRateScheduler reducing learning rate to 0.0001084442493013197.\n",
      "Epoch 27/40\n",
      "28/28 [==============================] - 32s 1s/step - loss: 0.0072 - categorical_accuracy: 0.9983\n",
      "\n",
      "Epoch 00028: LearningRateScheduler reducing learning rate to 0.00010675539944105576.\n",
      "Epoch 28/40\n",
      "28/28 [==============================] - 32s 1s/step - loss: 0.0059 - categorical_accuracy: 0.9989\n",
      "\n",
      "Epoch 00029: LearningRateScheduler reducing learning rate to 0.0001054043195528446.\n",
      "Epoch 29/40\n",
      "28/28 [==============================] - 32s 1s/step - loss: 0.0113 - categorical_accuracy: 0.9967\n",
      "\n",
      "Epoch 00030: LearningRateScheduler reducing learning rate to 0.00010432345564227568.\n",
      "Epoch 30/40\n",
      "28/28 [==============================] - 33s 1s/step - loss: 0.0073 - categorical_accuracy: 0.9989\n",
      "\n",
      "Epoch 00031: LearningRateScheduler reducing learning rate to 0.00010345876451382055.\n",
      "Epoch 31/40\n",
      "28/28 [==============================] - 34s 1s/step - loss: 0.0095 - categorical_accuracy: 0.9961\n",
      "\n",
      "Epoch 00032: LearningRateScheduler reducing learning rate to 0.00010276701161105644.\n",
      "Epoch 32/40\n",
      "28/28 [==============================] - 33s 1s/step - loss: 0.0053 - categorical_accuracy: 0.9989\n",
      "\n",
      "Epoch 00033: LearningRateScheduler reducing learning rate to 0.00010221360928884516.\n",
      "Epoch 33/40\n",
      "28/28 [==============================] - 33s 1s/step - loss: 0.0150 - categorical_accuracy: 0.9978\n",
      "\n",
      "Epoch 00034: LearningRateScheduler reducing learning rate to 0.00010177088743107613.\n",
      "Epoch 34/40\n",
      "28/28 [==============================] - 32s 1s/step - loss: 0.0081 - categorical_accuracy: 0.9978\n",
      "\n",
      "Epoch 00035: LearningRateScheduler reducing learning rate to 0.0001014167099448609.\n",
      "Epoch 35/40\n",
      "28/28 [==============================] - 32s 1s/step - loss: 0.0066 - categorical_accuracy: 0.9983\n",
      "\n",
      "Epoch 00036: LearningRateScheduler reducing learning rate to 0.00010113336795588872.\n",
      "Epoch 36/40\n",
      "28/28 [==============================] - 33s 1s/step - loss: 0.0072 - categorical_accuracy: 0.9972\n",
      "\n",
      "Epoch 00037: LearningRateScheduler reducing learning rate to 0.00010090669436471098.\n",
      "Epoch 37/40\n",
      "28/28 [==============================] - 33s 1s/step - loss: 0.0041 - categorical_accuracy: 0.9994\n",
      "\n",
      "Epoch 00038: LearningRateScheduler reducing learning rate to 0.00010072535549176879.\n",
      "Epoch 38/40\n",
      "28/28 [==============================] - 33s 1s/step - loss: 0.0056 - categorical_accuracy: 0.9978\n",
      "\n",
      "Epoch 00039: LearningRateScheduler reducing learning rate to 0.00010058028439341503.\n",
      "Epoch 39/40\n",
      "28/28 [==============================] - 33s 1s/step - loss: 0.0041 - categorical_accuracy: 0.9983\n",
      "\n",
      "Epoch 00040: LearningRateScheduler reducing learning rate to 0.00010046422751473202.\n",
      "Epoch 40/40\n",
      "28/28 [==============================] - 33s 1s/step - loss: 0.0041 - categorical_accuracy: 0.9989\n"
     ]
    }
   ],
   "source": [
    "STEPS_PER_EPOCH = train_labels.shape[0] // 64\n",
    "\n",
    "history = model2.fit(\n",
    "    train_dataset_1, \n",
    "    epochs=EPOCHS, \n",
    "    callbacks=[lr_callback],\n",
    "    steps_per_epoch=STEPS_PER_EPOCH,\n",
    "    #validation_data=valid_dataset\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_training_curves(training, validation, title, subplot):\n",
    "    \"\"\"\n",
    "    Source: https://www.kaggle.com/mgornergoogle/getting-started-with-100-flowers-on-tpu\n",
    "    \"\"\"\n",
    "    if subplot%10==1: # set up the subplots on the first call\n",
    "        plt.subplots(figsize=(10,10), facecolor='#F0F0F0')\n",
    "        plt.tight_layout()\n",
    "    ax = plt.subplot(subplot)\n",
    "    ax.set_facecolor('#F8F8F8')\n",
    "    ax.plot(training)\n",
    "    ax.plot(validation)\n",
    "    ax.set_title('model '+ title)\n",
    "    ax.set_ylabel(title)\n",
    "    #ax.set_ylim(0.28,1.05)\n",
    "    ax.set_xlabel('epoch')\n",
    "    ax.legend(['train', 'valid.'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'val_loss'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-62119d05cea1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m display_training_curves(\n\u001b[1;32m      2\u001b[0m     \u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val_loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     'loss', 211)\n\u001b[1;32m      5\u001b[0m display_training_curves(\n",
      "\u001b[0;31mKeyError\u001b[0m: 'val_loss'"
     ]
    }
   ],
   "source": [
    "display_training_curves(\n",
    "    history.history['loss'], \n",
    "    history.history['val_loss'], \n",
    "    'loss', 211)\n",
    "display_training_curves(\n",
    "    history.history['categorical_accuracy'], \n",
    "    history.history['val_categorical_accuracy'], \n",
    "    'accuracy', 212)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15/15 [==============================] - 135s 9s/step\n",
      "15/15 [==============================] - 75s 5s/step\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_id</th>\n",
       "      <th>healthy</th>\n",
       "      <th>multiple_diseases</th>\n",
       "      <th>rust</th>\n",
       "      <th>scab</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Test_0</td>\n",
       "      <td>1.480273e-07</td>\n",
       "      <td>5.736318e-05</td>\n",
       "      <td>9.999410e-01</td>\n",
       "      <td>1.478928e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Test_1</td>\n",
       "      <td>1.256120e-06</td>\n",
       "      <td>1.645207e-05</td>\n",
       "      <td>9.999800e-01</td>\n",
       "      <td>2.258512e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Test_2</td>\n",
       "      <td>1.455793e-06</td>\n",
       "      <td>3.042431e-06</td>\n",
       "      <td>1.798771e-07</td>\n",
       "      <td>9.999954e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Test_3</td>\n",
       "      <td>9.999899e-01</td>\n",
       "      <td>4.952619e-08</td>\n",
       "      <td>9.592408e-06</td>\n",
       "      <td>4.630330e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Test_4</td>\n",
       "      <td>1.766883e-05</td>\n",
       "      <td>7.575550e-04</td>\n",
       "      <td>9.992061e-01</td>\n",
       "      <td>1.867458e-05</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  image_id       healthy  multiple_diseases          rust          scab\n",
       "0   Test_0  1.480273e-07       5.736318e-05  9.999410e-01  1.478928e-06\n",
       "1   Test_1  1.256120e-06       1.645207e-05  9.999800e-01  2.258512e-06\n",
       "2   Test_2  1.455793e-06       3.042431e-06  1.798771e-07  9.999954e-01\n",
       "3   Test_3  9.999899e-01       4.952619e-08  9.592408e-06  4.630330e-07\n",
       "4   Test_4  1.766883e-05       7.575550e-04  9.992061e-01  1.867458e-05"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs1 = model.predict(test_dataset, verbose=1)\n",
    "probs2 = model2.predict(test_dataset, verbose=1)\n",
    "probs_avg = (probs1+probs2)/2\n",
    "sub.loc[:, 'healthy':] = probs_avg\n",
    "sub.to_csv('submission.csv', index=False)\n",
    "sub.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
