{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Similarity Modeling and Analysis with PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "This first section of the notebook corresponds to part 3 of the workshop\n",
    "\n",
    "-----\n",
    "## Workshop Part 3: Prepare the data and artifacts\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contents\n",
    "\n",
    "1. [Background](#Background)\n",
    "1. [Setup](#Setup)\n",
    "1. [Data](#Data)\n",
    "1. [Train](#Train)\n",
    "1. [Batch Inference](#Inference)\n",
    "\n",
    "---\n",
    "\n",
    "## Background\n",
    "\n",
    "In this notebook you will build a model to measure the similarity between products from the Zappos product catalog. This notebook uses the [UT Zappos50k](http://vision.cs.utexas.edu/projects/finegrained/utzap50k/) data set provided by University of Texas at Austin. The data has been processed and was scraped off the Zappos.com website, and is intended for non-commerical use.\n",
    "\n",
    "In this example, we will leverage a CNN based siamese network to learn the similarity function between the products in the catalog. The model is built on PyTorch and trained on SageMaker, and uses transfer learning techniques to leverage pre-trained ResNet models. Finally, the notebook demonstrates how to use SageMaker's Batch Inference functionality to produce a batch of similiarity measures between images using the trained siamese network.\n",
    "\n",
    "For more information about the PyTorch in SageMaker, please visit [sagemaker-pytorch-containers](https://github.com/aws/sagemaker-pytorch-containers) and [sagemaker-python-sdk](https://github.com/aws/sagemaker-python-sdk) Github repositories.\n",
    "\n",
    "---\n",
    "\n",
    "## Setup\n",
    "\n",
    "Let's start by creating a SageMaker session and specifying:\n",
    "\n",
    "- The S3 bucket and prefix that you want to use for training and model data.  This should be within the same region as the Notebook Instance, training, and hosting.\n",
    "- The IAM role arn used to give training and hosting access to your data. See [the documentation](https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-roles.html) for how to create these.  Note, if more than one role is required for notebook instances, training, and/or hosting, please replace the sagemaker.get_execution_role() with appropriate full IAM role arn string(s).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sagemaker\n",
    "\n",
    "SOURCE_DIR='source/similarity'\n",
    "WORKING_DIR = os.getcwd()\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "\n",
    "SAGEMAKER_BUCKET = sagemaker_session.default_bucket()\n",
    "SAGEMAKER_BUCKET_PREFIX = '/sagemaker/DEMO-pytorch-siamese-network/data'\n",
    "\n",
    "role = sagemaker.get_execution_role()\n",
    "DATA_S3URI = \"s3://\"+SAGEMAKER_BUCKET+SAGEMAKER_BUCKET_PREFIX\n",
    "\n",
    "print(\"Your SageMaker bucket: \"+SAGEMAKER_BUCKET)\n",
    "print(\"Current working directory: \"+WORKING_DIR)\n",
    "print(\"S3 location for storing training data: \" + DATA_S3URI)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 2\n",
    "\n",
    "Take note of your SageMaker bucket. You'll need to reference it later in this lab.\n",
    "\n",
    "Next, run this cell and take note of the region you're running this workshop in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "WORKSHOP_REGION = boto3.session.Session().region_name\n",
    "print(\"Your current region is \"+WORKSHOP_REGION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3\n",
    "\n",
    "For the next part of the workshop, work through the \"Data\" section of this notebook up until the next section \"Train.\"\n",
    "\n",
    "## Data\n",
    "\n",
    "The original dataset can be downloaded from [UT Zappos50k](http://vision.cs.utexas.edu/projects/finegrained/utzap50k/). The dataset has been made available at the following S3 bucket to offload the original site."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DOWNLOAD_S3URI = \"s3://reinvent2018-sagemaker-pytorch\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's preview one of the images from the data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash -s \"$DOWNLOAD_S3URI\" \"$WORKING_DIR\"\n",
    "\n",
    "mkdir -p $2/ut-zap50k-images-square/Boots/Knee\\ High/Anne\\ Klein/\n",
    "aws s3 cp $1/data/raw/Boots/Knee\\ High/Anne\\ Klein/8059298.310.jpg $2/ut-zap50k-images-square/Boots/Knee\\ High/Anne\\ Klein/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i> If you don't see an image generated by the next cell, you may have to run the cell again. </i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "img = Image.open(os.path.join(WORKING_DIR,\"ut-zap50k-images-square/Boots/Knee High/Anne Klein/8059298.310.jpg\"))\n",
    "plt.imshow(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uploading the data to S3\n",
    "\n",
    "We are going to transfer a subset of the Zappos50k dataset over to the default SageMaker bucket in your account. Training on the entire dataset will take hours on GPU as it involves training on a very large dataset of image pair combinations. The goal of this notebook is to demonstrate the process by training on a small subet of the images. \n",
    "\n",
    "All the images, processed inputs, and indexes for the entire dataset are publicly shared in the same S3 download location if you wish to experiment on the entire dataset. [Utility scripts](https://github.com/dylan-tong-aws/pytorch-image-similarity/blob/master/notebooks/pytorch_siamese_network_utils.ipynb) have also been made public for you to leverage if you wish to generate your own indexes and pre-processed dataset (npy tensors). \n",
    "\n",
    "First, we download a subset of sample images that is practical for the scope of the workshop. This should be sufficient to get a sense of the experience involved in the training and inference process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "SAMPLE_TRAINING_IMG_PATHS = \"sample_training_images.csv\"\n",
    "os.system(\"aws s3 cp \"+DOWNLOAD_S3URI+\"/data/sample/\"+SAMPLE_TRAINING_IMG_PATHS + \" \"+WORKING_DIR)\n",
    "df = pd.read_csv(os.path.join(WORKING_DIR,SAMPLE_TRAINING_IMG_PATHS), header=None, usecols=[0], names=['img'])\n",
    "IMG_PATHS = df['img'].tolist()\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "for paths in IMG_PATHS :\n",
    "    esc_path = paths.replace(' ','\\ ')\n",
    "    os.system('aws s3 sync ' + (DOWNLOAD_S3URI+\"/data/raw/\"+esc_path) +\n",
    "              ' '+ DATA_S3URI+'/'+esc_path +\n",
    "              ' --quiet --source-region us-west-2 --region '+ WORKSHOP_REGION)\n",
    "\n",
    "end = time.time()\n",
    "print(\"Sync completed in \"+ str(int(end - start))+\" seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PyTorch DataSet indexes\n",
    "\n",
    "Now we copy over index files that have been generated for you. These indexes will be used by the training script to locate selected images during training.\n",
    "\n",
    "The script expects indexes named as follows for the training and test sets respectively:\n",
    "-  zappos50k-tuples-index-train.csv\n",
    "-  zappos50k-tuples-index-test.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%bash -s \"$DOWNLOAD_S3URI\" \"$DATA_S3URI\" \"$WORKSHOP_REGION\"\n",
    "\n",
    "aws s3 cp $1/index/zappos50k-partial-tuples-index-test.csv $2/ --source-region us-west-2 --region $3\n",
    "aws s3 cp $1/index/zappos50k-partial-tuples-index-train.csv $2/ --source-region us-west-2 --region $3\n",
    "\n",
    "aws s3 mv $2/zappos50k-partial-tuples-index-train.csv $2/zappos50k-tuples-index-train.csv --quiet\n",
    "aws s3 mv $2/zappos50k-partial-tuples-index-test.csv $2/zappos50k-tuples-index-test.csv  --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-trained model artifacts\n",
    "\n",
    "Next, we copy over the artifacts for a pre-trained version of the model that we'll be creating in the following steps. This pre-trained model will be used later in the lab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODELS_S3URI = \"s3://\"+SAGEMAKER_BUCKET+'/sagemaker/DEMO-pytorch-siamese-network/model/output'\n",
    "SOURCE_S3URI = \"s3://\"+SAGEMAKER_BUCKET+'/sagemaker/DEMO-pytorch-siamese-network/model/source'\n",
    "MODEL_VERSION = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash -s \"$DOWNLOAD_S3URI\" \"$MODELS_S3URI\" \"$WORKSHOP_REGION\" \"$MODEL_VERSION\" \"$SOURCE_S3URI\"\n",
    "\n",
    "START_TIME=$SECONDS\n",
    "\n",
    "aws s3 sync $1/models/v$4 $2 --source-region us-west-2 --region $3 --quiet\n",
    "aws s3 cp $1/source/similarity/sourcedir.tar.gz $5/sourcedir.tar.gz --source-region us-west-2 --region $3 --quiet\n",
    "\n",
    "DURATION=$(($SECONDS - $START_TIME))\n",
    "\n",
    "echo\n",
    "echo Sync completed in $DURATION seconds\n",
    "echo\n",
    "echo Copy the following S3 URIs for your record. You will reference them later in the workshop:\n",
    "echo\n",
    "echo Pre-trained model artifacts: $2/model.tar.gz\n",
    "echo\n",
    "echo Pre-trained model source code: $5/sourcedir.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This concludes part 3 of the workshop. The next section of the notebook corresponds to part 4.\n",
    "\n",
    "----\n",
    "## Workshop Part 4: Train the model\n",
    "----\n",
    "\n",
    "## Train\n",
    "\n",
    "We need to provide a training script that can run on the SageMaker platform. The training script is very similar to a training script you might run outside of SageMaker, but you can access useful properties about the training environment through various environment variables.\n",
    "\n",
    "A typical training script loads data from the input channels, configures training with hyperparameters, trains a model, and saves a model to model_dir so that it can be hosted later. Hyperparameters are passed to your script as arguments and can be retrieved with an argparse.ArgumentParser instance.\n",
    "\n",
    "In the following steps, scripts are provided to build and train the following model:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep Siamese Network Architecture\n",
    "\n",
    "<img src=\"https://s3-us-west-2.amazonaws.com/reinvent2018-sagemaker-pytorch/web/siamese_network_diag.jpg\" width=\"65%\" height=\"65%\" align=\"left\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preview Results\n",
    "\n",
    "Before we build and train our model, let's preview some example results from a trained model.\n",
    "\n",
    "Download a subset of the Zappos50K dataset over to our Notebook instance along with some inference results generated from a pre-trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "SAMPLES_S3URI = DOWNLOAD_S3URI+'/data/sample/'\n",
    "\n",
    "SAMPLE_IMAGES_FILE = \"zappos50k-sample-images.zip\"\n",
    "SAMPLE_IMAGES_S3URI = SAMPLES_S3URI+SAMPLE_IMAGES_FILE\n",
    "\n",
    "OUTPUT_SAMPLES_FILE = \"sample-v2-batch-out.zip\"\n",
    "OUTPUT_SAMPLES_S3URI = SAMPLES_S3URI+\"batch-workshop/out/\"+OUTPUT_SAMPLES_FILE\n",
    "\n",
    "print('Downloading sample test images... \\n')\n",
    "os.system(\"aws s3 cp \"+SAMPLE_IMAGES_S3URI+\" \"+WORKING_DIR+\" --source-region us-west-2 --region \"+WORKSHOP_REGION)\n",
    "os.system(\"unzip ./\"+SAMPLE_IMAGES_FILE)\n",
    "\n",
    "sample_images_fp = os.path.join(WORKING_DIR,SAMPLE_IMAGES_FILE)\n",
    "if os.path.isfile(sample_images_fp) :\n",
    "    print(\"Samples test images were successfully downloaded to: \"+sample_images_fp)\n",
    "else :\n",
    "    print(\"Failed to download sample test images \\n\")\n",
    "\n",
    "print('\\nDownloading sample inference results... \\n')\n",
    "os.system(\"aws s3 cp \"+OUTPUT_SAMPLES_S3URI+\" \"+WORKING_DIR+\" --source-region us-west-2 --region \"+WORKSHOP_REGION)\n",
    "os.system(\"unzip ./\"+OUTPUT_SAMPLES_FILE)\n",
    "\n",
    "sample_inferences_fp = os.path.join(WORKING_DIR,OUTPUT_SAMPLES_FILE)\n",
    "if os.path.isfile(sample_inferences_fp) :\n",
    "    print(\"Samples inference results were successfully downloaded to: \"+sample_inferences_fp)\n",
    "else :\n",
    "    print(\"Failed to download sample inference results\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's load the sample outputs into dataframes..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "LOCAL_OUTPUT_DIR = os.path.join(WORKING_DIR,'out')\n",
    "LOCAL_DATA_DIR = os.path.join(WORKING_DIR,'data')\n",
    "SAMPLE_PRODUCTS = []\n",
    "\n",
    "def load_sample_outputs() :\n",
    "    i = 0\n",
    "    for (root, _, files) in os.walk(LOCAL_OUTPUT_DIR) :\n",
    "        for f in files :\n",
    "            fp = os.path.join(root,f)\n",
    "            df = pd.read_csv(fp, header=None, names=['img','sim'])\n",
    "            p=fp.replace(LOCAL_OUTPUT_DIR,'').replace('.csv','')\n",
    "            print(str(i)+\": \"+p)\n",
    "            SAMPLE_PRODUCTS.append({\"n\":p, \"sim\": df.sort_values(['sim'], ascending=True)})\n",
    "            i = i+1\n",
    "\n",
    "            import matplotlib.pyplot as plt\n",
    "    \n",
    "def display_similiar_products(product_info) :\n",
    "    count = 0\n",
    "    columns = 10\n",
    "    BATCHES_TO_DISPLAY = 3\n",
    "\n",
    "    fig = plt.figure(figsize=(columns*2, BATCHES_TO_DISPLAY*3))\n",
    "    fig.subplots_adjust(wspace=.1)\n",
    "\n",
    "    products = product_info['sim']\n",
    "    print(product_info['n'])\n",
    "    for (index, row) in products[0:columns*BATCHES_TO_DISPLAY].iterrows() :\n",
    "\n",
    "        count += 1\n",
    "        ax = fig.add_subplot(BATCHES_TO_DISPLAY, columns, count)   \n",
    "        ax.set_title(round(row['sim'],5))\n",
    "        ax.set_xticklabels([])\n",
    "        ax.set_yticklabels([])\n",
    "        plt.imshow(plt.imread(os.path.join(LOCAL_DATA_DIR,row['img'])))\n",
    "        \n",
    "    plt.show()\n",
    "\n",
    "load_sample_outputs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Samples\n",
    "\n",
    "The output from the cell above lists 7 products, which have pre-generated inferences. Run the cell below to view visually similar products. The first image is the product we're querying, and the products that follow are ordered by similarity (ordered from left to right and top to bottom in descending similarity). Change the index value from 0-6 if you like to preview other products.\n",
    "\n",
    "The images displayed are in ascending order of similarity (left to right and top to bottom). The value above the images represent an unnormalized similarity score. The first image is the product we're using to query for other stylistically similar products. It's value isn't exactly zero due to floating-point precision and the fact that this model represents a generalized approximation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_similiar_products(SAMPLE_PRODUCTS[5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download the provided scripts\n",
    "\n",
    "Let's move on to building and training a model. First, we download the scripts:\n",
    "\n",
    "-  cnn.py: this script implements a siamese network in PyTorch. It uses a pre-trained ResNet model, and replaces the last layer with a dense layer with the number of dimensions as specified by class parameters. All layers except the last have been frozen for training.\n",
    "\n",
    "-  siamese.py: contains all the training logic. The siamese network takes batches of image pairs, uses the CNN to extract salient features and vectorize the images, and minimizes a contrastive loss function according to the L1 distances between the image vectors and the provided labels.\n",
    "\n",
    "-  requirements.txt: describes the dependencies that are required by the scripts that aren't pre-installed on the SageMaker PyTorch container. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash -s \"$DOWNLOAD_S3URI\" \"$SOURCE_DIR\" \"$WORKSHOP_REGION\"\n",
    "\n",
    "mkdir -p $2\n",
    "aws s3 cp $1/$2/cnn.py ./$2 --source-region us-west-2 --region $3\n",
    "aws s3 cp $1/$2/siamese.py ./$2 --source-region us-west-2 --region $3\n",
    "aws s3 cp $1/$2/requirements.txt ./$2 --source-region us-west-2 --region $3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Script\n",
    "\n",
    "Run the cell below if you like to view the script for the PyTorch model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pygmentize 'source/similarity/cnn.py'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Script\n",
    "\n",
    "Run the cell below if you like to view the script for training the siamese network on PyTorch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pygmentize 'source/similarity/siamese.py'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters\n",
    "\n",
    "Next we define the hyperparameters for our training job. The configurations are set to train on a single epoch. In practice, you'll train for a longer period to obtain better results. In this lab, our goal is to be familiarized with the process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PARAM_EPOCHS = 1\n",
    "PARAM_BATCH_SIZE= 64\n",
    "PARAM_LR = 1e-6\n",
    "PARAM_SIMILARITY_DIMS = 64\n",
    "PARAM_SIM_MARGIN = 0.03\n",
    "PARAM_OPTIMIZER = 'Adam'\n",
    "PARAM_BEST_MODEL_METRIC = 'test-loss'\n",
    "\n",
    "HYPERPARAMETERS={\n",
    "                    'epochs': PARAM_EPOCHS,\n",
    "                    'batch-size': PARAM_BATCH_SIZE,\n",
    "                    'learning-rate': PARAM_LR,\n",
    "                    'similarity-dims': PARAM_SIMILARITY_DIMS,\n",
    "                    'similarity-margin': PARAM_SIM_MARGIN,\n",
    "                    'optimizer': PARAM_OPTIMIZER,\n",
    "                    'best-model-metric' : PARAM_BEST_MODEL_METRIC\n",
    "                }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the PyTorch Estimator\n",
    "\n",
    "The provided custom PyTorch training script can now be trained on SageMaker training servers without further setup. A PyTorch estimator needs to be instantiated as shown below specifying the training script to run, hyperparameters, and the infrastructure to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.pytorch import PyTorch\n",
    "\n",
    "def default_training_instancetype(region) :\n",
    "    \n",
    "    inst_map = {\n",
    "        'us-west-2': 'ml.p3.2xlarge',\n",
    "        'us-east-2': 'ml.p3.2xlarge',\n",
    "        'eu-west-1': 'ml.p3.2xlarge',\n",
    "        'ap-southeast-2': 'ml.p2.xlarge',\n",
    "        'ap-northeast-1': 'ml.p3.2xlarge'\n",
    "    }\n",
    "    \n",
    "    try :\n",
    "        \n",
    "        default = inst_map[region]\n",
    "        \n",
    "    except :\n",
    "        print(\"you're not running in one of the regions designated for this workshop!\")\n",
    "        \n",
    "    return default\n",
    "    \n",
    "estimator = PyTorch(entry_point=\"siamese.py\",\n",
    "                    role=role,\n",
    "                    framework_version='0.4.0',\n",
    "                    train_instance_count=1,\n",
    "                    train_instance_type=default_training_instancetype(WORKSHOP_REGION),\n",
    "                    source_dir=SOURCE_DIR,\n",
    "                    hyperparameters=HYPERPARAMETERS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the Estimator\n",
    "\n",
    "The training job is started simply by calling fit() on the estimator, and specifying the location of our training data and indexes. Note that you could specify a different location for your test set, but this isn't necessary as the provided indexes are used to seperate the training and test sets within the provided data corpus.  \n",
    "\n",
    "    If you train the model without changing the dataset subset or the default parameters provided, the training will complete in 10-20 minutes using a ml.p3.2xlarge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator.fit({'train':DATA_S3URI})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "##### The core workshop ends here. Do not wait for the training job to complete. Return to the [guide](https://s3-us-west-2.amazonaws.com/reinvent2018-sagemaker-pytorch/guides/Amazon+SageMaker+PyTorch+%26+Analytics+Workshop.pdf) and proceed to the next part of the workshop where you'll be working from the SageMaker console.\n",
    "---\n",
    "\n",
    "## -- OPTIONAL CONTENT ---\n",
    "\n",
    "Only experiment with the contents below after you have completed the core workshop. The optional content extends well beyond the scope and allocated time for this workshop. The content is for those who like to venture deeper into this project and SageMaker after the completion of the workshop."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional: Programmatic Batch Inference\n",
    "\n",
    "The workshop guide illustrates the process of launching a batch inference job from the console. The section below illustrates how this can be done programmatically.\n",
    "\n",
    "Run the cell below to download the script for batch inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash -s \"$DOWNLOAD_S3URI\" \"$SOURCE_DIR\"\n",
    "mkdir -p $2\n",
    "aws s3 cp $1/$2/batch.py ./$2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the cell below if you like to view the batch inference script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pygmentize 'source/similarity/batch.py'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set some constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_INPUT_S3URI = DATA_S3URI+\"/batch/in\"\n",
    "BATCH_OUTPUT_S3URI = DATA_S3URI+\"/batch/out\"\n",
    "BATCH_MODEL_NAME = \"batch-zappos50k-siamese-cnn\"\n",
    "BATCH_INSTANCE_TYPE = 'ml.c5.xlarge'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The batch inference job that we're going to create expects inputs in NPY format. The images have been pre-processed by packing pairs of image tensors in NPY format into gzip files, so that images can be sent to a SageMaker Batch Transform job cluster in batches. \n",
    "\n",
    "Pre-processed images have been provided for you. Run the cell to download them to your SageMaker bucket, so that your batch inference job has access to the input data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash -s \"$DOWNLOAD_S3URI\" \"$BATCH_INPUT_S3URI\" \"$WORKSHOP_REGION\"\n",
    "\n",
    "start=$SECONDS\n",
    "aws s3 sync $1/data/sample/batch-workshop/in $2 --quiet --source-region us-west-2 --region $3\n",
    "duration=$(( SECONDS - start ))\n",
    "\n",
    "echo sync completed in $duration seconds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Register a Model for Batch Inference\n",
    "\n",
    "We need to register a model with SageMaker for the batch inference job. This model uses the batch inference script as the entry point. We'll use the pre-trained model that was downloaded in the previous steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.pytorch import PyTorchModel\n",
    "\n",
    "batchModel = PyTorchModel(model_data=MODELS_S3URI+'/model.tar.gz', \n",
    "                            role=role,\n",
    "                            framework_version='0.4.0',\n",
    "                            entry_point='batch.py',\n",
    "                            source_dir=SOURCE_DIR)\n",
    "\n",
    "batchModel.sagemaker_session = sagemaker_session\n",
    "container_def = batchModel.prepare_container_def(instance_type=BATCH_INSTANCE_TYPE)\n",
    "sagemaker_session.create_model(BATCH_MODEL_NAME, role, container_def)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Launch the Batch Inference Job\n",
    "\n",
    "Now we're ready to run a Batch Inference job. Run the snippet below to launch a batch inference job programmatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.transformer import Transformer\n",
    "\n",
    "transformer = Transformer(model_name=BATCH_MODEL_NAME,\n",
    "                          instance_count=1,\n",
    "                          instance_type= BATCH_INSTANCE_TYPE,\n",
    "                          accept = 'text/csv',\n",
    "                          output_path=BATCH_OUTPUT_S3URI\n",
    "                         )\n",
    "transformer.transform(BATCH_INPUT_S3URI, content_type= 'application/x-npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional: Real-time Inference\n",
    "\n",
    "We can also deploy models in SageMaker for real-time inference. Let's create an endpoint that takes two image tensors and returns the similarity measure between the images as a json output.\n",
    "\n",
    "First, download the inference script, which will be used as the entry point for a real-time inference model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash -s \"$DOWNLOAD_S3URI\" \"$SOURCE_DIR\"\n",
    "mkdir -p $2\n",
    "aws s3 cp $1/$2/inference.py ./$2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the cell below if you like to view the real-time inference script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pygmentize 'source/similarity/batch.py'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run this cell to set a name for our real-time inference endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RT_ENDPOINT_NAME = \"rt-zappos50k-siamese-cnn\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deploy Endpoint\n",
    "\n",
    "Run the following code to register a model, and deploy our endpoint. Note that the configurations utilize the pre-trained model that was downloaded previously.\n",
    "\n",
    "The model uses the inference.py script as it's entry point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sagemaker.pytorch import PyTorchModel\n",
    "from sagemaker.predictor import RealTimePredictor, npy_serializer, json_deserializer\n",
    "\n",
    "rtModel = PyTorchModel(model_data=MODELS_S3URI+'/model.tar.gz', \n",
    "                            role=role,\n",
    "                            framework_version='0.4.0',\n",
    "                            entry_point='inference.py',\n",
    "                            source_dir=SOURCE_DIR)\n",
    "\n",
    "rtPredictor = rtModel.deploy(instance_type='ml.c5.xlarge', \n",
    "                                   initial_instance_count=1,\n",
    "                                   endpoint_name=RT_ENDPOINT_NAME)\n",
    "                      \n",
    "rtPredictor = RealTimePredictor(endpoint=RT_ENDPOINT_NAME,\n",
    "                                 serializer=npy_serializer,\n",
    "                                 deserializer= json_deserializer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use an Existing Endpoint\n",
    "\n",
    "If you already deployed an real-time endpoint previously, you can run the following code snippet instead of the prevous snippet to acquire a reference to the existing endpoint instead of deploying a new one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.predictor import RealTimePredictor, npy_serializer, json_deserializer\n",
    "        \n",
    "rtPredictor = RealTimePredictor(endpoint=RT_ENDPOINT_NAME,\n",
    "                                 serializer=npy_serializer,\n",
    "                                 deserializer= json_deserializer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the Endpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, run the cell below to initialize some constants and utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "\n",
    "TRANSFORMATIONS = \\\n",
    "transforms.Compose([\n",
    "    transforms.Resize(224), \\\n",
    "    transforms.ToTensor(), \\\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],std=[0.229, 0.224, 0.225]) \\\n",
    "])\n",
    "\n",
    "\n",
    "PRODUCT = \"Boots/Knee High/Anne Klein/8059298.310.jpg\"\n",
    "#PRODUCT = \"Shoes/Oxfords/Bass/7563706.3.jpg\"\n",
    "#PRODUCT = \"Shoes/Oxfords/Rockport/7996677.6194.jpg\"\n",
    "#PRODUCT = \"Shoes/Oxfords/Calvin Klein/7943176.325.jpg\"\n",
    "#PRODUCT = \"Boots/Over the Knee/Calvin Klein Collection/8005712.365488.jpg\"\n",
    "#PRODUCT = \"Boots/Knee High/Anne Klein/8032745.10224.jpg\"\n",
    "#PRODUCT = \"Boots/Knee High/Ariat/7992449.16158.jpg\"\n",
    "#PRODUCT = \"Sandals/Heel/Fly Flot/7418709.9.jpg\"\n",
    "#PRODUCT = Shoes/Oxfords/Bass/7563706.3.jpg\"\n",
    "\n",
    "IMG1_LOC = os.path.join(WORKING_DIR,\"data/\"+PRODUCT)\n",
    "ZAPPOS50K_SAMPLE_INDEX = \"zappos50k-partial-index.csv\"\n",
    "\n",
    "os.system(\"aws s3 cp \"+DOWNLOAD_S3URI+\"/index/\"+ZAPPOS50K_SAMPLE_INDEX+\" \"+\n",
    "      os.path.join(WORKING_DIR,ZAPPOS50K_SAMPLE_INDEX) + \n",
    "      \" --source-region us-west-2 --region \"+ WORKSHOP_REGION)\n",
    "\n",
    "def getImageTensor(img_path, transform):\n",
    "    \n",
    "    image = Image.open(img_path)\n",
    "    image_tensor = transform(image)\n",
    "        \n",
    "    return image_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's run a quick test comparing an image with itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "img1 = getImageTensor(IMG1_LOC, TRANSFORMATIONS)\n",
    "imgPair= np.vstack((img1,img1))\n",
    "res= rtPredictor.predict(imgPair)\n",
    "\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following snippet to display the similar products for our selection in order of most similar.\n",
    "\n",
    "This is a real-time endpoint and the following code will make api calls a few hundred times comparing the selected product (image) to a few hundred others to capture similarity measures. It will take 5-6 minutes to execute. In practice, you can scale-out (optionally, with auto-scaling) to increase the throughput on the endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "img1 = getImageTensor(IMG1_LOC, TRANSFORMATIONS)\n",
    "df = pd.read_csv(ZAPPOS50K_SAMPLE_INDEX, header=None, usecols=[0], names=['img'])    \n",
    "\n",
    "for (i,r) in df.iterrows() :\n",
    "    \n",
    "    img2 = getImageTensor(LOCAL_DATA_DIR+'/'+r['img'], TRANSFORMATIONS)\n",
    "    imgPair= np.vstack((img1,img2))\n",
    "    res= rtPredictor.predict(imgPair)\n",
    "    df.at[i,'sim'] = float(res['similarity'])\n",
    "\n",
    "df= df.sort_values(['sim'], ascending=True)\n",
    "\n",
    "pd.set_option(\"display.max_rows\",10)\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize\n",
    "\n",
    "Let's visualize the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_similiar_products({\"n\": IMG1_LOC, \"sim\": df})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional: Automatic Model Tuning \n",
    "\n",
    "Hyperparameters can have substantial effect on the performance and outcome of your model training. A training process traditionally involves a strategy for exploring a range of hyperparameter values in search for a combination that delivers optimal results. \n",
    "\n",
    "SageMaker offers a capability call Automatic Model Tuning that performs the process of finding optimal hyperparameters for you by running training jobs in parallel for you and using a Bayesian search strategy to narrow down on the optimal combination of hyperparameters. Ultimately, this reduces the amount of training runs compared to basic hyperparameter search strategies like random search that most data scientists typically use to find optimal hyperparameters, and can lead to discovering parameters that lead to superior results.\n",
    "\n",
    "The snippets below shows how to launch an hyperparameter tuning job programmatically. You are free to change the base hyperparameters and ranges if you wish. The settings below limit the parallel training jobs to one as it is configured to use GPU instances, and by default, AWS accounts only allow you to launch one of these instance types at a time. If your account has been approved for higher limits, and you are prepared to pay for the training time, you can change the parameters to launch multiple instances and jobs running in parallel to speed up the hyperparameter search process. The default sample datasets with the configurations below will require 40-60 minutes per training job, and run up to a maximum of 30 jobs. You can stop the tuning job from the console to end the process prematurely.\n",
    "\n",
    "Run the cell below to initialize the base hyperparameters and estimator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.tuner import IntegerParameter, CategoricalParameter, ContinuousParameter\n",
    "\n",
    "PARAM_EPOCHS = 8\n",
    "PARAM_BATCH_SIZE= 64\n",
    "PARAM_LR = 1e-4\n",
    "PARAM_SIMILARITY_DIMS = 64\n",
    "PARAM_SIM_MARGIN = 0.03\n",
    "PARAM_OPTIMIZER = 'Adam'\n",
    "\n",
    "HYPERPARAMETERS={\n",
    "                    'epochs': PARAM_EPOCHS,\n",
    "                    'batch-size': PARAM_BATCH_SIZE,\n",
    "                    'learning-rate': PARAM_LR,\n",
    "                    'similarity-dims': PARAM_SIMILARITY_DIMS,\n",
    "                    'similarity-margin': PARAM_SIM_MARGIN,\n",
    "                    'optimizer': PARAM_OPTIMIZER\n",
    "                }\n",
    "\n",
    "tuned_estimator = PyTorch(entry_point=\"siamese.py\",\n",
    "                            role=role,\n",
    "                            framework_version='0.4.0',\n",
    "                            train_instance_count=1,\n",
    "                            train_instance_type=default_training_instancetype(WORKSHOP_REGION),\n",
    "                            source_dir=SOURCE_DIR,\n",
    "                            base_job_name='HPO-pytorch',\n",
    "                            hyperparameters=HYPERPARAMETERS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Hyperparameter Ranges\n",
    "\n",
    "Hyperparameter tuning jobs are different from standard training jobs in the sense that you provide the job information about the range of hyperparameters you like to search. The snippet below demonstrates this idea. In this case, we'll explore multiple settings for the training learning rate, the number of dimensions configured in the last layer of the CNN as well as the type of optimizer to use (Adam vs. plain Stochastic Gradient Descent).\n",
    "\n",
    "Lastly, an objective metric needs to be defined to determine what is the best model. The model that delivers the best results for the defined objective metric is what is ultimately delivered by the tuning job. The configurations below delivers the model and settings that result in the lowest \"average training loss.' You are free to define whatever metric you like. For instance, in many cases, validation accuracy is likely what you're optimizing for.\n",
    "\n",
    "This hyperparameter process runs up to 2 jobs sequentially. The time required to complete the HPO job will range from 3-6 hours depending on the instance type used. The instance type varies depending on the region you're running this lab in. In practice, you will also run more than 2 jobs, and to speed up the process, you will run multiple jobs in parallel. Running more parallel jobs will require contacting AWs Support to increasing the default soft limit for SageMaker p-family instances on your account. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.tuner import HyperparameterTuner\n",
    "\n",
    "HYPERPARAM_RANGES = {\n",
    "                        'learning-rate': ContinuousParameter(1e-6, 1e-4),\n",
    "                        'similarity-dims': CategoricalParameter([16,32,64,96,128]),\n",
    "                        'optimizer': CategoricalParameter(['Adam','SGD'])\n",
    "                    }\n",
    "\n",
    "OBJECTIVE_METRIC_NAME = 'average training loss'\n",
    "METRIC_DEFINITIONS = [{'Name': 'average training loss',\n",
    "                       'Regex': 'Training set: Average loss: ([0-9\\\\.]+)'}]\n",
    "\n",
    "# Configure HyperparameterTuner\n",
    "tuner = HyperparameterTuner(estimator=tuned_estimator,\n",
    "                            objective_metric_name = OBJECTIVE_METRIC_NAME,\n",
    "                            hyperparameter_ranges  = HYPERPARAM_RANGES,\n",
    "                            metric_definitions = METRIC_DEFINITIONS,\n",
    "                            max_jobs=2,\n",
    "                            max_parallel_jobs=1)\n",
    "\n",
    "# Start hyperparameter tuning job\n",
    "tuner.fit({'train': DATA_S3URI})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deploy the Model\n",
    "\n",
    "You can deploy the best model returned by the tuning job programatically in the same way you would deploy a model produced by a basic training job.\n",
    "\n",
    "In this case, the model is deployed as a real-time endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.pytorch import PyTorchModel\n",
    "from sagemaker.predictor import RealTimePredictor, npy_serializer, json_deserializer\n",
    "\n",
    "TUNED_MODEL_DATA = \"s3://\"+SAGEMAKER_BUCKET+\"/\"+tuner.best_training_job()+\"/output/model.tar.gz\"\n",
    "RT_TUNED_ENDPOINT_NAME = 'HPO-rt-zappos50k'\n",
    "\n",
    "tuned_model = PyTorchModel(model_data=TUNED_MODEL_DATA, \n",
    "                            role=role,\n",
    "                            framework_version='0.4.0',\n",
    "                            entry_point='inference.py',\n",
    "                            source_dir=SOURCE_DIR)\n",
    "\n",
    "tuned_predictor = tuned_model.deploy(instance_type='ml.c5.xlarge', \n",
    "                                   initial_instance_count=1,\n",
    "                                   endpoint_name=RT_TUNED_ENDPOINT_NAME)\n",
    "                      \n",
    "tuned_predictor = RealTimePredictor(endpoint=RT_TUNED_ENDPOINT_NAME,\n",
    "                                 serializer=npy_serializer,\n",
    "                                 deserializer= json_deserializer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the Model\n",
    "\n",
    "Below are some constants and utilities for testing this model. You can change the value of PRODUCT if you like to visualize the similar products for something different from what is set below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "\n",
    "TRANSFORMATIONS = \\\n",
    "transforms.Compose([\n",
    "    transforms.Resize(224), \\\n",
    "    transforms.ToTensor(), \\\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],std=[0.229, 0.224, 0.225]) \\\n",
    "])\n",
    "\n",
    "\n",
    "PRODUCT = \"Boots/Knee High/Anne Klein/8059298.310.jpg\"\n",
    "\n",
    "IMG1_LOC = os.path.join(WORKING_DIR,\"data/\"+PRODUCT)\n",
    "ZAPPOS50K_SAMPLE_INDEX = \"zappos50k-partial-index.csv\"\n",
    "\n",
    "os.system(\"aws s3 cp \"+DOWNLOAD_S3URI+\"/index/\"+ZAPPOS50K_SAMPLE_INDEX+\" \"+\n",
    "      os.path.join(WORKING_DIR,ZAPPOS50K_SAMPLE_INDEX) + \n",
    "      \" --source-region us-west-2 --region \"+ WORKSHOP_REGION)\n",
    "\n",
    "def getImageTensor(img_path, transform):\n",
    "    \n",
    "    image = Image.open(img_path)\n",
    "    image_tensor = transform(image)\n",
    "        \n",
    "    return image_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display the results\n",
    "\n",
    "Run the following snippet to display the similar products for our selection in order of most similar.\n",
    "\n",
    "This is a real-time endpoint and the following code will make api calls a few hundred times comparing the selected product (image) to a few hundred others to capture similarity measures. It will take 5-6 minutes to execute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "img1 = getImageTensor(IMG1_LOC, TRANSFORMATIONS)\n",
    "df = pd.read_csv(ZAPPOS50K_SAMPLE_INDEX, header=None, usecols=[0], names=['img'])    \n",
    "\n",
    "for (i,r) in df.iterrows() :\n",
    "    \n",
    "    img2 = getImageTensor(LOCAL_DATA_DIR+'/'+r['img'], TRANSFORMATIONS)\n",
    "    imgPair= np.vstack((img1,img2))\n",
    "    res= tuned_predictor.predict(imgPair)\n",
    "    df.at[i,'sim'] = float(res['similarity'])\n",
    "\n",
    "df= df.sort_values(['sim'], ascending=True)\n",
    "\n",
    "pd.set_option(\"display.max_rows\",10)\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize\n",
    "\n",
    "Let's visualize the results.\n",
    "\n",
    "* Note that the tuned results probably won't be as good as the provided pre-trained model. The pre-trained model has been trained on the entire Zappos50K data set and on days worth of training time. The training on this lab is only on a subset of the data set. It also uses a [slightly different training algorithm than the one in the lab](https://github.com/dylan-tong-aws/pytorch-image-similarity/blob/master/src/similarity/siamese2.py). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_similiar_products({\"n\": IMG1_LOC, \"sim\": df})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p36",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
