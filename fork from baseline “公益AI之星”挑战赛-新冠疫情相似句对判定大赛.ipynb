{"cells":[{"cell_type":"code","source":"# 查看数据文件目录  list datalab files\n!ls datalab/","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 查看个人永久空间文件  list files in your permanent storage\n!ls /home/tianchi/myspace/\n代码修改自bert4keras。预训练模型chinese wwm 祝大家取得好成绩。\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 查看当前kernel下已安装的包  list packages\n!pip list --format=columns","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 绘图案例 an example of matplotlib\n%matplotlib inline\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.special import jn\nfrom IPython.display import display, clear_output\nimport time\nx = np.linspace(0,5)\nf, ax = plt.subplots()\nax.set_title(\"Bessel functions\")\n\nfor n in range(1,10):\n    time.sleep(1)\n    ax.plot(x, jn(x,n))\n    clear_output(wait=True)\n    display(f)\n\n# close the figure at the end, so we don't get a duplicate\n# of the last plot\nplt.close()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from keras.layers import *\n\nfrom bert4keras.backend import keras, set_gelu\nfrom bert4keras.bert import build_bert_model\nfrom bert4keras.optimizers import Adam\nfrom bert4keras.snippets import sequence_padding, DataGenerator\nfrom bert4keras.tokenizer import Tokenizer\nimport pandas as pd\nimport numpy as np\n\nset_gelu('tanh')  # 切换gelu版本\n\nmaxlen = 32\nbatch_size = 16\nconfig_path = 'publish/bert_config.json'\ncheckpoint_path = 'publish/bert_model.ckpt'\ndict_path = 'publish/vocab.txt'\n\n\ndef load_data(filename):\n    D = pd.read_csv(filename).values.tolist()\n    return D\n\n\n# 加载数据集\nall_data = load_data('训练数据名称.csv')\nrandom_order = range(len(all_data))\nnp.random.shuffle(list(random_order))\ntrain_data = [all_data[j] for i, j in enumerate(random_order) if i % 6 != 1 and i%6!=2]\nvalid_data = [all_data[j] for i, j in enumerate(random_order) if i % 6 == 1]\ntest_data = [all_data[j] for i, j in enumerate(random_order) if i % 6 == 2]\n# 建立分词器\ntokenizer = Tokenizer(dict_path, do_lower_case=True)\n\n\nclass data_generator(DataGenerator):\n    \"\"\"数据生成器\n    \"\"\"\n\n    def __iter__(self, random=False):\n        idxs = list(range(len(self.data)))\n        if random:\n            np.random.shuffle(idxs)\n        batch_token_ids, batch_segment_ids, batch_labels = [], [], []\n        for i in idxs:\n            text1, text2, label = self.data[i]\n#             print(text1, text2, label)\n            token_ids, segment_ids = tokenizer.encode(text1, text2, max_length=maxlen)\n            batch_token_ids.append(token_ids)\n            batch_segment_ids.append(segment_ids)\n            batch_labels.append([label])\n            if len(batch_token_ids) == self.batch_size or i == idxs[-1]:\n                batch_token_ids = sequence_padding(batch_token_ids)\n                batch_segment_ids = sequence_padding(batch_segment_ids)\n                batch_labels = sequence_padding(batch_labels)\n                yield [batch_token_ids, batch_segment_ids], batch_labels\n                batch_token_ids, batch_segment_ids, batch_labels = [], [], []\n\n\n# 加载预训练模型\nbert = build_bert_model(\n    config_path=config_path,\n    checkpoint_path=checkpoint_path,\n    with_pool=True,\n    return_keras_model=False,\n)\n\noutput = Dropout(rate=0.1)(bert.model.output)\noutput = Dense(units=2,\n               activation='softmax',\n               kernel_initializer=bert.initializer)(output)\n\nmodel = keras.models.Model(bert.model.input, output)\nmodel.summary()\n\nmodel.compile(\n    loss='sparse_categorical_crossentropy',\n    optimizer=Adam(2e-5),  # 用足够小的学习率\n    # optimizer=PiecewiseLinearLearningRate(Adam(5e-5), {10000: 1, 30000: 0.1}),\n    metrics=['accuracy'],\n)\n\n# 转换数据集\ntrain_generator = data_generator(train_data, batch_size)\nvalid_generator = data_generator(valid_data, batch_size)\ntest_generator = data_generator(test_data, batch_size)\n\n\ndef evaluate(data):\n    total, right = 0., 0.\n    for x_true, y_true in data:\n        y_pred = model.predict(x_true).argmax(axis=1)\n        y_true = y_true[:, 0]\n        total += len(y_true)\n        right += (y_true == y_pred).sum()\n    return right / total\n\n\nclass Evaluator(keras.callbacks.Callback):\n    def __init__(self):\n        self.best_val_acc = 0.\n\n    def on_epoch_end(self, epoch, logs=None):\n        val_acc = evaluate(valid_generator)\n        if val_acc > self.best_val_acc:\n            self.best_val_acc = val_acc\n            model.save_weights('best_model.weights')\n        test_acc = evaluate(test_generator)\n        print(u'val_acc: %.5f, best_val_acc: %.5f, test_acc: %.5f\\n'\n              % (val_acc, self.best_val_acc, test_acc))\n\n\nevaluator = Evaluator()\nmodel.fit_generator(train_generator.forfit(),\n                    steps_per_epoch=len(train_generator),\n                    epochs=20,\n                    callbacks=[evaluator])\n\nmodel.load_weights('best_model.weights')\nprint(u'final test acc: %05f\\n' % (evaluate(test_generator)))","metadata":{},"execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.5"}},"nbformat":4,"nbformat_minor":2}